{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Spark introduction\n",
    "<br></br>\n",
    "<center>\n",
    "<img src=\"../images/ibm-logo-bw.png\" alt=\"ibm-logo\" align=\"center\" style=\"width: 200px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from termcolor import cprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 11\n",
    "LARGE_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=LARGE_SIZE)   # fontsize of the figure title\n",
    "\n",
    "def slide_print(text, color='white'):\n",
    "    cprint(text, color, 'on_grey')\n",
    "    \n",
    "%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Sometimes a single machine simple cannot perform a given task fast enough.  Sometimes there are too many tasks for a single machine to properly handle and yet othertimes there are so much data that the data must be distributed across resources.  These scenarios describe several of the situations where the use of Apache Spark has the potential to directly impact a business opportunity.  Whether you are working with spark locally, on Watson Studio, from within Docker or as part of a computer cluster the basics will cover in this video will still apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## High performance computing\n",
    "\n",
    "* <span style=\"color:orange\">Symmetric multiprocessing</span> -Two or more identical processors connected to a single unit of memory.\n",
    "* <span style=\"color:orange\">Distributed computing</span> - Processing elements are connected by a network.\n",
    "* <span style=\"color:orange\">Cluster computing</span> - Group of coupled computers that work together in a way that they can be viewed as a single system.\n",
    "* <span style=\"color:orange\">Massive parallel processing</span> - Many networked processors usually > 100 used to perform computations in parallel.\n",
    "* <span style=\"color:orange\">Grid computing</span> - Distributed computing making use of a middle layer to create a `virtual super computer`.\n",
    "\n",
    "> Spark is a cluster-computing framework. When you compare it to hadoop it essentially competes with which the MapReduce component of the Hadoop ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There are many types of high performance computing environments.  READ STATEMENT.  Spark does not have its own distributed filesystem, but can use the Hadoop Distributed File System or HDFS.  Spark uses memory and can use disk for processing, whereas MapReduce has strictly disk-based processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark Applications (overview)\n",
    "\n",
    "<br></br>\n",
    "<center>\n",
    "<img src=\"../images/spark-arch.png\" alt=\"spark-arch\" align=\"center\" style=\"width: 600px;\"/>\n",
    "</center>\n",
    "\n",
    "> Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here we have a diagram of Spark Application.  When we start a spark environment a Spark Session is first created and this manages the driver process.  The driver program can be controlled using API's in Scala, Python SQL, Java and R. The worker nodes on the right are usually distinct machines. Executors are the worker nodes' processes, each in charge of running and individual task and they are shown as the orange squares on the worker nodes. One cluster configuration is to assign several cores to each executor leaving one for additional overhead.  The cluster manager, shown on the bottom, which is often YARN, Mesos or Kubernetes helps coordinate between the driver program and worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark Applications (general process)\n",
    "\n",
    "<br></br>\n",
    "<center>\n",
    "<img src=\"../images/spark-arch.png\" alt=\"spark-arch\" align=\"center\" style=\"width: 600px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Spark applications are run as an independent sets of processes on a cluster coordinated by the SparkContext object in your main program. Each application gets its own executor processes, which remains allocated for the duration of application.  The driver program, that encapsulates both the SparkContext and the SparkSession is used to submit Spark Applications in Spark .The driver program, once it is given instructions in the form of user code will then ask the cluster manager to launch executors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SparkSession\n",
    "\n",
    "* SparkContext\n",
    "* SQLContext\n",
    "* HiveContext\n",
    "* StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\u001b[37m2.4.5\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.executor.memory', '512m')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.driver.port', '33333')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.mount.readOnly', 'false')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.mount.path', '/root/data')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.executor.id', 'driver')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.kubernetes.pyspark.pythonVersion', '3')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.driver.memory', '512m')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.kubernetes.namespace', 'spark')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.app.name', 'spark')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.kubernetes.authenticate.serviceAccountName', 'spark')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.rdd.compress', 'True')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.serializer.objectStreamReset', '100')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.executor.instances', '1')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.kubernetes.container.image', 'sbc1/spark-py')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.executor.cores', '1')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.submit.deployMode', 'client')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.master', 'k8s://https://kubernetes.default.svc.cluster.local:443')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.app.id', 'spark-application-1588434508628')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.driver.host', 'my-notebook-deployment.spark.svc.cluster.local')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.options.claimName', 'my-notebook-pvc')\u001b[0m\n",
      "\u001b[40m\u001b[37m('spark.ui.showConsoleProgress', 'true')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# spark = (ps.sql.SparkSession.builder\n",
    "#         .appName(\"sandbox\")\n",
    "#         .getOrCreate()\n",
    "#         )\n",
    "\n",
    "# sparkConf = ps.SparkConf()\n",
    "# sparkConf.setMaster(\"k8s://https://localhost:6443\")\n",
    "# sparkConf.setAppName(\"KUBERNETES-IS-AWESOME\")\n",
    "# sparkConf.set(\"spark.kubernetes.container.image\", \"robot108/spark-py:latest\")\n",
    "# sparkConf.set(\"spark.kubernetes.namespace\", \"playground\")\n",
    "\n",
    "# spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# Create Spark config for our Kubernetes based cluster manager\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "sparkConf.setAppName(\"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"sbc1/spark-py\")\n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "sparkConf.set(\"spark.executor.instances\", \"1\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\")\n",
    "sparkConf.set(\"spark.driver.port\", \"33333\")\n",
    "sparkConf.set(\"spark.driver.host\", \"my-notebook-deployment.spark.svc.cluster.local\")\n",
    "sparkConf.set(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.options.claimName\",\"my-notebook-pvc\")\n",
    "sparkConf.set(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.mount.path\",\"/root/data\")\n",
    "sparkConf.set(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.mount.readOnly\",\"false\")\n",
    "# Initialize our Spark cluster, this will actually\n",
    "# generate the worker nodes.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "slide_print(spark.version)\n",
    "\n",
    "for attribute in sc._conf.getAll():\n",
    "    slide_print(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Prior to Spark 2.0 there were multiple points of entry for a Spark application, including: SparkContext, SQLContext, HiveContext, and the StreamingContext.  More recent versions of Spark combine all of these objects into a single point of entry that can be used for all Spark applications.  The SparkContext is a child process of the SparkSession. In this cell first we create a SparkSession using the SparkSession builder then we show how to access the Spark Context. Using the SparkContext we show here how to print some of the configuration properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RDD (Resilient Distributed Dataset) \n",
    "\n",
    "### An example workflow\n",
    "\n",
    "1. Create the enviromnent to run spark from python\n",
    "2. Extract RDDs or DataFrames from files\n",
    "3. Carry out transformations\n",
    "4. Execute actions to obtain values (local objects in python)\n",
    "\n",
    "### The RDD API has two types of operations:\n",
    "\n",
    "* <span style=\"color:orange\">Transformations</span> - Define a new dataset based on a previous one\n",
    "* <span style=\"color:orange\">Actions</span> - launch a job for execution on a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\u001b[37m<class 'pyspark.rdd.RDD'>\u001b[0m\n",
      "\u001b[40m\u001b[37m[('', 3136), ('Project', 81), ('The', 272), ('of', 2723), ('Arthur', 14), ('Conan', 4), ('is', 1074)]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text_file = sc.textFile(\"./sherlock-holmes.txt\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "slide_print(type(text_file))  # show that the data are read into an RDD\n",
    "results = counts.collect()    # output the content in python (CAUTION)\n",
    "slide_print(results[:7])      # print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Theses four enumerated steps represent an example workflow.  The code version of these steps are shown just below, with the exception of setting up the environment which we just did.  Spark revolves around the concept of resilient distributed datasets or RDDs.  An RDD is a fault-tolerant collection of elements that can be operated on in parallel. The RDD API, uses two types types of operations: transformations and and actions.  On top of Sparkâ€™s RDD API, high level APIs are provided, including the DataFrame API and Machine Learning API, both of which we will cover in this course.  The text_file is an RDD that once subjected to a chain of transformations is used to create counts, another RDD.  The action we use here is 'collect', which brings the data back into Python and it is something you should exercise caution with, especially when working with very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To keep in mind\n",
    "\n",
    " * Transformations are the main way of expressing business logic in Spark.\n",
    " * Spark doesn't apply the transformation right away, it just builds on the DAG\n",
    " * Transformations can be chained together\n",
    " *  RDDs are an immutable distributed collection of elements of your data, partitioned across nodes in your cluster that can be operated on in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total divisible by 3: 166936/500000\n"
     ]
    }
   ],
   "source": [
    "# an RDD from a an array\n",
    "rand_nums = np.random.randint(0,500,500000)\n",
    "rdd = sc.parallelize(rand_nums) \n",
    "divis_by_3 = rdd.filter(lambda x: x % 3 == 0)\n",
    "\n",
    "total1 = rdd.count()\n",
    "total2 = divis_by_3.count()\n",
    "\n",
    "print(\"total divisible by 3: {}/{}\".format(total2,total1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There are several important things to remember about RDDs.  The first is that they use what is known as lazy evaluation.  This means that Spark will wait until the very last moment to execute your transformations.  It does this by constructing a Direct Acyclic Graph or DAG of the transformations.  When an action like count or collect is called the task is sent for execution.  Here we show another example on an RDD, but this time we create it using a numpy array with the parallelize function.  The transformation is a simple filter and the actions are obtain counts rather than all of the data since we generally want to avoid pulling all data into local memory unless we have too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark-submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-04851e9af54b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"pi\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "# %%writefile calculate-pi.py\n",
    "\n",
    "import pyspark as ps\n",
    "import random\n",
    "\n",
    "# spark = (ps.sql.SparkSession.builder\n",
    "#         .appName(\"get-pi\")\n",
    "#         .getOrCreate()\n",
    "#         )\n",
    "# ,\n",
    "# sc = spark.sparkContext\n",
    "# random.seed(1)\n",
    "\n",
    "def sample(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "rdd = sc.parallelize(range(0, 10000000)) \\\n",
    "           .map(sample) \\\n",
    "           .reduce(lambda a, b: a + b)\n",
    "count = rdd.count()\n",
    "\n",
    "result = {\"pi\": (4.0 * count / 10000000)}\n",
    "print(result, file=open('calculate-pi-out.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```bash\n",
    "~$ spark-submit calculate-pi.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\u001b[37m{'pi': 3.1414548}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "with open(\"calculate-pi-out.txt\") as file:  \n",
    "    results = file.read() \n",
    "    results = ast.literal_eval(results)\n",
    "    \n",
    "slide_print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Applications can be submitted to a cluster of any type using the command spark-submit and an accompanying script. The writefile magic function shown here saved the code in the cell as a python script to be called by spark-submit.  The file calculate-pi, unsurprisingly calculates pi, but more importantly it shows an example of how to map a custom function over an RDD.  Spark-submit can be run from the command line as shown.  Generally, it is called with a number of options using a bash file, but it can be called using all the defaults as shown here. Once the script is run it will produce the outfile that is being printed here.  This procedure could be used to make a prediction using a machine learning model that has been tuned, trained and saved. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
